{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at gpt2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "cannot access local variable 'dataset' where it is not associated with a value",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 65\u001b[0m\n\u001b[0;32m     63\u001b[0m dir_path \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mC:/Users/serda/OneDrive\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mBureau\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mOnline Education/Certification/Projet final/Projetbooks/test_clean_10books\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m### or if you use google drive link to find a book in the 'clean' folder in drive\u001b[39;00m\n\u001b[0;32m     64\u001b[0m text_paths \u001b[39m=\u001b[39m [os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(dir_path, f) \u001b[39mfor\u001b[39;00m f \u001b[39min\u001b[39;00m os\u001b[39m.\u001b[39mlistdir(dir_path) \u001b[39mif\u001b[39;00m f\u001b[39m.\u001b[39mendswith(\u001b[39m\"\u001b[39m\u001b[39m.txt\u001b[39m\u001b[39m\"\u001b[39m)] \u001b[39m### Attention this will train all files .txt write explicit name of the book is better.\u001b[39;00m\n\u001b[1;32m---> 65\u001b[0m train_dataset, num_examples \u001b[39m=\u001b[39m dataset_loader(text_paths, tokenizer)\n\u001b[0;32m     67\u001b[0m \u001b[39m# Configure training\u001b[39;00m\n\u001b[0;32m     68\u001b[0m optimizer \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39moptimizers\u001b[39m.\u001b[39mAdam(learning_rate\u001b[39m=\u001b[39m\u001b[39m5e-5\u001b[39m) \u001b[39m# learning rate -try as you wish\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[3], line 34\u001b[0m, in \u001b[0;36mdataset_loader\u001b[1;34m(text_paths, tokenizer)\u001b[0m\n\u001b[0;32m     32\u001b[0m options \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mOptions()\n\u001b[0;32m     33\u001b[0m options\u001b[39m.\u001b[39mexperimental_deterministic \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m---> 34\u001b[0m dataset \u001b[39m=\u001b[39m dataset\u001b[39m.\u001b[39mwith_options(options)\n\u001b[0;32m     37\u001b[0m num_examples \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(encoded_texts)\n\u001b[0;32m     39\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgenerator\u001b[39m():\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m: cannot access local variable 'dataset' where it is not associated with a value"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import tensorflow as tf\n",
    "from transformers import TFGPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# Load the pre-trained model and tokenizer from Hugging Face Transformers library\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model = TFGPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def paragraph_separater(teller):\n",
    "    paragraphs = re.split(r'\\n{2,}', teller) ### this is a paragraph seperator\n",
    "    return paragraphs\n",
    "\n",
    "def separated_text_groups(paragraphs, group_size=10):  ### group them as you wish but I advise 10\n",
    "    grouped_paragraphs = []\n",
    "    for i in range(0, len(paragraphs), group_size):\n",
    "        group = ' '.join(paragraphs[i:i + group_size])\n",
    "        grouped_paragraphs.append(group)\n",
    "    return grouped_paragraphs\n",
    "\n",
    "def dataset_loader(text_paths, tokenizer):  ###this will open your book(s) and tokenizer\n",
    "    encoded_texts = []\n",
    "    for text_path in text_paths:\n",
    "        with open(text_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            teller = f.read()\n",
    "        paragraphs = paragraph_separater(teller)  ###tokenizers as above are up to HuggingFace Transformers Library\n",
    "        for paragraph in paragraphs:\n",
    "            encoded_text = tokenizer(paragraph, return_tensors=\"tf\", max_length=128, truncation=True, padding=\"max_length\")\n",
    "            encoded_texts.append(encoded_text)\n",
    "    \n",
    "    options = tf.data.Options()\n",
    "    options.experimental_deterministic = True\n",
    "    dataset = dataset.with_options(options)\n",
    "\n",
    "\n",
    "    num_examples = len(encoded_texts)\n",
    "\n",
    "    def generator():\n",
    "        for encoded_text in encoded_texts:\n",
    "            yield {\n",
    "                \"input_ids\": encoded_text[\"input_ids\"][0],\n",
    "                \"attention_mask\": encoded_text[\"attention_mask\"][0],\n",
    "                \"labels\": encoded_text[\"input_ids\"][0],\n",
    "            }\n",
    "\n",
    "    dataset = tf.data.Dataset.from_generator(\n",
    "        generator,\n",
    "        output_signature={\n",
    "            \"input_ids\": tf.TensorSpec(shape=(128,), dtype=tf.int32),\n",
    "            \"attention_mask\": tf.TensorSpec(shape=(128,), dtype=tf.int32),\n",
    "            \"labels\": tf.TensorSpec(shape=(128,), dtype=tf.int32),\n",
    "        }\n",
    "    )\n",
    "\n",
    "    options = tf.data.Options()\n",
    "    options.experimental_deterministic = False\n",
    "    dataset = dataset.with_options(options)\n",
    "\n",
    "    return dataset, num_examples\n",
    "\n",
    "# Load dataset\n",
    "dir_path = \"C:/Users/serda/OneDrive\\Bureau\\Online Education/Certification/Projet final/Projetbooks/test_clean_10books\" ### or if you use google drive link to find a book in the 'clean' folder in drive\n",
    "text_paths = [os.path.join(dir_path, f) for f in os.listdir(dir_path) if f.endswith(\".txt\")] ### Attention this will train all files .txt write explicit name of the book is better.\n",
    "train_dataset, num_examples = dataset_loader(text_paths, tokenizer)\n",
    "\n",
    "# Configure training\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5) # learning rate -try as you wish\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "model.compile(optimizer=optimizer, loss=loss)\n",
    "\n",
    "# Fine-tune the model\n",
    "batch_size = 32 ### try from 8 -to -128 and observe the loss\n",
    "num_epochs = 3 # attention this may take really long! 3 not enough but it is up to your time!\n",
    "steps_per_epoch = num_examples // batch_size  ### to compute how many steps rested in following\n",
    "model.fit(train_dataset.batch(batch_size), epochs=num_epochs, steps_per_epoch=steps_per_epoch)\n",
    "\n",
    "\n",
    "# Save the fine-tuned model\n",
    "model.save_pretrained(\"./gpt2_finetuned\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at gpt2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "cannot access local variable 'dataset' where it is not associated with a value",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 65\u001b[0m\n\u001b[0;32m     63\u001b[0m dir_path \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mC:/Users/serda/OneDrive\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mBureau\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mOnline Education/Certification/Projet final/Projetbooks/test_clean_10books\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m### or if you use google drive link to find a book in the 'clean' folder in drive\u001b[39;00m\n\u001b[0;32m     64\u001b[0m text_paths \u001b[39m=\u001b[39m [os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(dir_path, f) \u001b[39mfor\u001b[39;00m f \u001b[39min\u001b[39;00m os\u001b[39m.\u001b[39mlistdir(dir_path) \u001b[39mif\u001b[39;00m f\u001b[39m.\u001b[39mendswith(\u001b[39m\"\u001b[39m\u001b[39m.txt\u001b[39m\u001b[39m\"\u001b[39m)] \u001b[39m### Attention this will train all files .txt write explicit name of the book is better.\u001b[39;00m\n\u001b[1;32m---> 65\u001b[0m train_dataset, num_examples \u001b[39m=\u001b[39m dataset_loader(text_paths, tokenizer)\n\u001b[0;32m     67\u001b[0m \u001b[39m# Configure training\u001b[39;00m\n\u001b[0;32m     68\u001b[0m optimizer \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39moptimizers\u001b[39m.\u001b[39mAdam(learning_rate\u001b[39m=\u001b[39m\u001b[39m5e-5\u001b[39m) \u001b[39m# learning rate -try as you wish\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[4], line 34\u001b[0m, in \u001b[0;36mdataset_loader\u001b[1;34m(text_paths, tokenizer)\u001b[0m\n\u001b[0;32m     32\u001b[0m options \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mOptions()\n\u001b[0;32m     33\u001b[0m options\u001b[39m.\u001b[39mexperimental_deterministic \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m---> 34\u001b[0m dataset \u001b[39m=\u001b[39m dataset\u001b[39m.\u001b[39mwith_options(options)\n\u001b[0;32m     37\u001b[0m num_examples \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(encoded_texts)\n\u001b[0;32m     39\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgenerator\u001b[39m():\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m: cannot access local variable 'dataset' where it is not associated with a value"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import tensorflow as tf\n",
    "from transformers import TFGPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# Load the pre-trained model and tokenizer from Hugging Face Transformers library\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model = TFGPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def paragraph_separater(teller):\n",
    "    paragraphs = re.split(r'\\n{2,}', teller) ### this is a paragraph seperator\n",
    "    return paragraphs\n",
    "\n",
    "def separated_text_groups(paragraphs, group_size=10):  ### group them as you wish but I advise 10\n",
    "    grouped_paragraphs = []\n",
    "    for i in range(0, len(paragraphs), group_size):\n",
    "        group = ' '.join(paragraphs[i:i + group_size])\n",
    "        grouped_paragraphs.append(group)\n",
    "    return grouped_paragraphs\n",
    "\n",
    "def dataset_loader(text_paths, tokenizer):  ###this will open your book(s) and tokenizer\n",
    "    encoded_texts = []\n",
    "    for text_path in text_paths:\n",
    "        with open(text_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            teller = f.read()\n",
    "        paragraphs = paragraph_separater(teller)  ###tokenizers as above are up to HuggingFace Transformers Library\n",
    "        for paragraph in paragraphs:\n",
    "            encoded_text = tokenizer(paragraph, return_tensors=\"tf\", max_length=128, truncation=True, padding=\"max_length\")\n",
    "            encoded_texts.append(encoded_text)\n",
    "    \n",
    "    options = tf.data.Options()\n",
    "    options.experimental_deterministic = True\n",
    "    dataset = dataset.with_options(options)\n",
    "\n",
    "\n",
    "    num_examples = len(encoded_texts)\n",
    "\n",
    "    def generator():\n",
    "        for encoded_text in encoded_texts:\n",
    "            yield {\n",
    "                \"input_ids\": encoded_text[\"input_ids\"][0],\n",
    "                \"attention_mask\": encoded_text[\"attention_mask\"][0],\n",
    "                \"labels\": encoded_text[\"input_ids\"][0],\n",
    "            }\n",
    "\n",
    "    dataset = tf.data.Dataset.from_generator(\n",
    "        generator,\n",
    "        output_signature={\n",
    "            \"input_ids\": tf.TensorSpec(shape=(128,), dtype=tf.int32),\n",
    "            \"attention_mask\": tf.TensorSpec(shape=(128,), dtype=tf.int32),\n",
    "            \"labels\": tf.TensorSpec(shape=(128,), dtype=tf.int32),\n",
    "        }\n",
    "    )\n",
    "\n",
    "    options = tf.data.Options()\n",
    "    options.experimental_deterministic = False\n",
    "    dataset = dataset.with_options(options)\n",
    "\n",
    "    return dataset, num_examples\n",
    "\n",
    "# Load dataset\n",
    "dir_path = \"C:/Users/serda/OneDrive\\Bureau\\Online Education/Certification/Projet final/Projetbooks/test_clean_10books\" ### or if you use google drive link to find a book in the 'clean' folder in drive\n",
    "text_paths = [os.path.join(dir_path, f) for f in os.listdir(dir_path) if f.endswith(\".txt\")] ### Attention this will train all files .txt write explicit name of the book is better.\n",
    "train_dataset, num_examples = dataset_loader(text_paths, tokenizer)\n",
    "\n",
    "# Configure training\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5) # learning rate -try as you wish\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "model.compile(optimizer=optimizer, loss=loss)\n",
    "\n",
    "# Fine-tune the model\n",
    "batch_size = 32 ### try from 8 -to -128 and observe the loss\n",
    "num_epochs = 3 # attention this may take really long! 3 not enough but it is up to your time!\n",
    "steps_per_epoch = num_examples // batch_size  ### to compute how many steps rested in following\n",
    "model.fit(train_dataset.batch(batch_size), epochs=num_epochs, steps_per_epoch=steps_per_epoch)\n",
    "\n",
    "\n",
    "# Save the fine-tuned model\n",
    "model.save_pretrained(\"./gpt2_finetuned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
