{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import tensorflow as tf\n",
    "from transformers import TFGPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# Load the pre-trained model and tokenizer from Hugging Face Transformers library\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model = TFGPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def paragraph_separater(teller):\n",
    "    paragraphs = re.split(r'\\n{2,}', teller) ### this is a paragraph seperator\n",
    "    return paragraphs\n",
    "\n",
    "def separated_text_groups(paragraphs, group_size=10):  ### group them as you wish but I advise 10\n",
    "    grouped_paragraphs = []\n",
    "    for i in range(0, len(paragraphs), group_size):\n",
    "        group = ' '.join(paragraphs[i:i + group_size])\n",
    "        grouped_paragraphs.append(group)\n",
    "    return grouped_paragraphs\n",
    "\n",
    "def dataset_loader(text_paths, tokenizer):  ###this will open your book(s) and tokenizer\n",
    "    encoded_texts = []\n",
    "    for text_path in text_paths:\n",
    "        with open(text_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            teller = f.read()\n",
    "        paragraphs = paragraph_separater(teller)  ###tokenizers as above are up to HuggingFace Transformers Library\n",
    "        for paragraph in paragraphs:\n",
    "            encoded_text = tokenizer(paragraph, return_tensors=\"tf\", max_length=128, truncation=True, padding=\"max_length\")\n",
    "            encoded_texts.append(encoded_text)\n",
    "\n",
    "    num_examples = len(encoded_texts)\n",
    "\n",
    "    def generator():\n",
    "        for encoded_text in encoded_texts:\n",
    "            yield {\n",
    "                \"input_ids\": encoded_text[\"input_ids\"][0],\n",
    "                \"attention_mask\": encoded_text[\"attention_mask\"][0],\n",
    "                \"labels\": encoded_text[\"input_ids\"][0],\n",
    "            }\n",
    "\n",
    "    dataset = tf.data.Dataset.from_generator(\n",
    "        generator,\n",
    "        output_signature={\n",
    "            \"input_ids\": tf.TensorSpec(shape=(128,), dtype=tf.int32),\n",
    "            \"attention_mask\": tf.TensorSpec(shape=(128,), dtype=tf.int32),\n",
    "            \"labels\": tf.TensorSpec(shape=(128,), dtype=tf.int32),\n",
    "        }\n",
    "    )\n",
    "\n",
    "    options = tf.data.Options()\n",
    "    options.experimental_deterministic = False\n",
    "    dataset = dataset.with_options(options)\n",
    "\n",
    "    return dataset, num_examples\n",
    "\n",
    "# Load dataset\n",
    "dir_path = \"C:/your local path \" ### or if you use google drive link to find a book in the 'clean' folder in drive\n",
    "text_paths = [os.path.join(dir_path, f) for f in os.listdir(dir_path) if f.endswith(\".txt\")] ### Attention this will train all files .txt write explicit name of the book is better.\n",
    "train_dataset, num_examples = dataset_loader(text_paths, tokenizer)\n",
    "\n",
    "# Configure training\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5) # learning rate -try as you wish\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "model.compile(optimizer=optimizer, loss=loss)\n",
    "\n",
    "# Fine-tune the model\n",
    "batch_size = 32 ### try from 8 -to -128 and observe the loss\n",
    "num_epochs = 3 # attention this may take really long! 3 not enough but it is up to your time!\n",
    "steps_per_epoch = num_examples // batch_size  ### to compute how many steps rested in following\n",
    "model.fit(train_dataset.batch(batch_size), epochs=num_epochs, steps_per_epoch=steps_per_epoch)\n",
    "\n",
    "\n",
    "# Save the fine-tuned model\n",
    "model.save_pretrained(\"./gpt2_finetuned\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
