{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78306b27",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at gpt2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "he was in a ship, and his body was carried off.\n",
      "\n",
      "He was the first person to die while in the water, he said.\n",
      "1: he was in a ship, and his body was carried off.\n",
      "\n",
      "He was the first person to die while in the water, he said.\n",
      "2: he was in a ship, and his body was carried off.\n",
      "\n",
      "He was the first person to die while in the water, he said. The others were taken to hospitals, where they have been treated for dehydration.\n",
      "3: he was in a ship, and his body was carried off.\n",
      "\n",
      "He was the first person to die while in the water, he said. He was also one of the last people to escape the ship.\n",
      "invalid literal for int() with base 10: 'he went to house'\n",
      "\n",
      "he was in a ship with his sister. She said she was afraid for her life.\n",
      "1: he was in a ship with his sister. She said she was afraid for her life. \"I thought I was going to die and I would die,\" she recalled.\n",
      "\n",
      "She went to the hospital, where\n",
      "2: he was in a ship with his sister. She said she was afraid for her life. \"I thought I was going to die and I would die,\" she recalled.\n",
      "\n",
      "She went to the hospital, and\n",
      "3: he was in a ship with his sister. She said she was afraid for her life. \"I thought I was going to die and I would die,\" she recalled.\n",
      "\n",
      "She went to the hospital and told\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import TFGPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model = TFGPT2LMHeadModel.from_pretrained('C:/Users/serda/OneDrive/Bureau/Online Education/Certification/Projet final/gpt2_finetuned')\n",
    "\n",
    "def generate_text(prompt, max_tokens=25, temperature=1.5, num_return_sequences=1):\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"tf\")\n",
    "    attention_mask = tf.ones_like(input_ids, dtype=tf.int32)\n",
    "\n",
    "    output = model.generate(\n",
    "        input_ids,\n",
    "        max_length=input_ids.shape[1] + max_tokens,\n",
    "        temperature=temperature,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        attention_mask=attention_mask,\n",
    "        top_k=75,\n",
    "        top_p=0.94,\n",
    "        num_return_sequences=num_return_sequences,\n",
    "        num_beams=5, \n",
    "        no_repeat_ngram_size=2, \n",
    "        early_stopping=True,\n",
    "        do_sample=True,\n",
    "    )\n",
    "\n",
    "    return [tokenizer.decode(out, skip_special_tokens=True) for out in output]\n",
    "\n",
    "def generate_choices(prompt, num_choices=3):\n",
    "    choices = generate_text(prompt, num_return_sequences=num_choices)\n",
    "    return choices\n",
    "\n",
    "def display_story_segment(segment, choices):\n",
    "    print(\"\\n\" + segment)\n",
    "    for i, choice in enumerate(choices, start=1):\n",
    "        print(f\"{i}: {choice}\")\n",
    "\n",
    "def handle_user_input(prompt, choice, choices):\n",
    "    if 1 <= choice <= len(choices):\n",
    "        return prompt + \" \" + choices[choice - 1]\n",
    "    else:\n",
    "        raise ValueError(\"Invalid choice. Please enter a valid number.\")\n",
    "\n",
    "def main():\n",
    "    story = input(\"Enter the first sentence of the story: \")\n",
    "    max_words = 1000\n",
    "    word_count = 0\n",
    "\n",
    "    while word_count < max_words:\n",
    "        segment = generate_text(story)[-1]\n",
    "        word_count += len(segment.split())\n",
    "        choices = generate_choices(segment)\n",
    "        display_story_segment(segment, choices)\n",
    "\n",
    "        user_choice = input(\"\\nEnter your choice (1, 2, or 3): \")\n",
    "        try:\n",
    "            choice = int(user_choice)\n",
    "            story = handle_user_input(story, choice, choices)\n",
    "        except ValueError as e:\n",
    "            print(e)\n",
    "            continue\n",
    "\n",
    "        user_input = input(\"\\nEnter a question or input for the next paragraph (or type 'end of story' or 'end of film' to finish): \")\n",
    "        if user_input.lower() in ['end of story', 'end of film']:\n",
    "            break\n",
    "\n",
    "        story += \" \" + user_input\n",
    "\n",
    "    print(\"\\nFinal story:\")\n",
    "    print(story)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77795644",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
