{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at gpt2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "512/512 [==============================] - 7464s 15s/step - loss: 0.1164\n",
      "Epoch 2/3\n",
      "  1/512 [..............................] - ETA: 1:04:28 - loss: 0.1763WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 1536 batches). You may need to use the repeat() function when building your dataset.\n",
      "512/512 [==============================] - 10s 5ms/step - loss: 0.1763\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import tensorflow as tf\n",
    "from transformers import GPT2Tokenizer, TFGPT2LMHeadModel\n",
    "\n",
    "def sentence_tokenizer(text):\n",
    "    sentences = re.split(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s', text)\n",
    "    return sentences\n",
    "\n",
    "def dataset_loader(text_paths, tokenizer):\n",
    "    encoded_texts = []\n",
    "    for text_path in text_paths:\n",
    "        with open(text_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            text = f.read()\n",
    "        sentences = sentence_tokenizer(text)\n",
    "        for sentence in sentences:\n",
    "            encoded_text = tokenizer(sentence, return_tensors=\"tf\", max_length=128, truncation=True, padding=\"max_length\")\n",
    "            encoded_texts.append(encoded_text)\n",
    "\n",
    "    num_examples = len(encoded_texts)\n",
    "\n",
    "    def generator():\n",
    "        for encoded_text in encoded_texts:\n",
    "            yield {\n",
    "                \"input_ids\": encoded_text[\"input_ids\"][0],\n",
    "                \"attention_mask\": encoded_text[\"attention_mask\"][0],\n",
    "                \"labels\": encoded_text[\"input_ids\"][0],\n",
    "            }\n",
    "\n",
    "    dataset = tf.data.Dataset.from_generator(\n",
    "        generator,\n",
    "        output_signature={\n",
    "            \"input_ids\": tf.TensorSpec(shape=(128,), dtype=tf.int32),\n",
    "            \"attention_mask\": tf.TensorSpec(shape=(128,), dtype=tf.int32),\n",
    "            \"labels\": tf.TensorSpec(shape=(128,), dtype=tf.int32),\n",
    "        }\n",
    "    )\n",
    "\n",
    "    options = tf.data.Options()\n",
    "    options.experimental_deterministic = False\n",
    "    dataset = dataset.with_options(options)\n",
    "\n",
    "    return dataset, num_examples\n",
    "\n",
    "# Instantiate the tokenizer and the model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model = TFGPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Load dataset\n",
    "dir_path = r\"C:\\Users\\serda\\OneDrive\\Bureau\\Online Education\\Certification\\Projet final\\Projetbooks\\clean\"\n",
    "text_paths = [os.path.join(dir_path, \"cleaned_annakarenina.txt\")]\n",
    "train_dataset, num_examples = dataset_loader(text_paths, tokenizer)\n",
    "\n",
    "# Configure training\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "model.compile(optimizer=optimizer, loss=loss)\n",
    "\n",
    "# Fine-tune the model\n",
    "batch_size = 32\n",
    "num_epochs = 3\n",
    "steps_per_epoch = num_examples // batch_size\n",
    "model.fit(train_dataset.batch(batch_size), epochs=num_epochs, steps_per_epoch=steps_per_epoch)\n",
    "\n",
    "# Save the fine-tuned model\n",
    "model.save_pretrained(\"./gpt2_finetuned\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
